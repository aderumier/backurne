#!/usr/bin/python3

import atexit
import datetime
import dateutil.parser
import multiprocessing
import requests
import sys

import pretty
from config import config
from log import log as Log
from ceph import Ceph
from proxmox import Proxmox
from restore import Restore
from backup import Bck


# High level class, used for check-related stuff
class Check():
	def __init__(self, cluster):
		self.cluster = cluster
		self.err = list()

	def add_err(self, msg):
		if msg is None:
			return
		msg = '%s: %s' % (self.cluster['name'], msg)
		self.err.append(msg)

	def check_img(self, args):
		try:
			ceph = args['ceph']
			backup = args['backup']
			rbd = args['image']

			if not ceph.backup.exists(backup.dest):
				msg = 'No backup found for %s (image does not exists)' % (rbd,)
				return msg

			last = ceph.get_last_shared_snap(rbd, backup.dest)
			if last is None:
				msg = 'No backup found for %s (no shared snap)' % (rbd,)
				return msg

			when = last.split(';')[3]
			when = dateutil.parser.parse(when)
			deadline = datetime.timedelta(days=1) + datetime.timedelta(hours=6)
			deadline = datetime.datetime.now() - deadline
			if when < deadline:
				msg = 'Backup found for %s, yet too old (created at %s)' % (rbd, when)
				return msg
		except Exception as e:
			Log.warn('Exception thrown while checking %s : %s' % (args, e))

	def cmp_snap(self, backup, ceph, rbd):
		live_snaps = ceph.snap(rbd)
		try:
			backup_snaps = ceph.backup.snap(backup.dest)
		except:
			backup_snaps = []
		inter = list(set(live_snaps).intersection(backup_snaps))
		for snap in inter:
			live = ceph.checksum(rbd, snap)
			back = ceph.backup.checksum(backup.dest, snap)
			if live == back:
				continue

			self.add_err('ERR: shared snapshot %s does not match' % (snap,))
			self.add_err('\tOn live (image: %s): %s' % (rbd, live))
			self.add_err('\tOn backup (image: %s): %s' % (backup.dest, back))


class CheckProxmox(Check):
	def __init__(self, cluster):
		super().__init__(cluster)
		self.px = Proxmox(cluster)

	def check(self):
		data = list()
		for vm in self.px.vms():
			for disk in vm['to_backup']:
				ceph = self.px.ceph_storage[disk['ceph']]
				bck = Bck(disk['ceph'], ceph, disk['rbd'], vm=vm, adapter=disk['adapter'])
				data.append({'ceph': ceph, 'backup': bck, 'image': disk['rbd']})

		self.err = list()
		with multiprocessing.Pool() as pool:
			for msg in pool.imap_unordered(self.check_img, data):
				self.add_err(msg)

		return self.err

	def check_snap(self):
		for vm in self.px.vms():
			for disk in vm['to_backup']:
				ceph = self.px.ceph_storage[disk['ceph']]
				bck = Bck(disk['ceph'], ceph, disk['rbd'], vm=vm, adapter=disk['adapter'])
				self.cmp_snap(bck, ceph, disk['rbd'])
		return self.err


class CheckPlain(Check):
	def __init__(self, cluster):
		super().__init__(cluster)
		self.ceph = Ceph(self.cluster['pool'], endpoint=self.cluster['fqdn'])

	def check(self):
		for rbd in self.ceph.ls():
			bck = Bck(self.cluster['name'], self.ceph, rbd)
			self.check_img(self.ceph, bck, rbd)
		return self.err

	def check_snap(self):
		for rbd in self.ceph.ls():
			bck = Bck(self.cluster['name'], self.ceph, rbd)
			self.cmp_snap(bck, self.ceph, rbd)
		return self.err


class Bidule():
	# On this module, an "item" is either:
	# - a vm object, for proxmox
	# - a rbd image, for plain
	def __init__(self, cluster, queue):
		self.cluster = cluster
		self.queue = queue
		if cluster['type'] == 'proxmox':
			self.px = Proxmox(self.cluster)
		else:
			self.ceph = Ceph(self.cluster['pool'], endpoint=self.cluster['fqdn'])

	def __fetch_profiles(self, vm, disk):
		profiles = list(config['profiles'].items())

		if config['profiles_api'] is None:
			return profiles

		try:
			json = {
				'cluster': {
					'type': 'proxmox',
					'name': self.cluster['name'],
					'fqdn': self.cluster['fqdn'],
				},
				'vm': {
					'vmid': vm['vmid'],
					'name': vm['name'],
				},
				'disk': disk,
			}

			add = requests.post(config['profiles_api'], json=json)
			add.raise_for_status()
			add = add.json()

			if 'backup' in add and add['backup'] is False:
				return list()

			if 'profiles' in add:
				profiles += list(add['profiles'].items())

		except Exception as e:
			Log.warn('Exception thrown while fetching profiles for %s : %s' % (vm, e))
		return profiles

	def list_items(self):
		if self.cluster['type'] != 'proxmox':
			return self.ceph.ls()

		vms = self.px.vms()

		result = list()
		for vm in vms:
			if vm['smbios'] is None and self.cluster['use_smbios'] is True:
				if config['uuid_fallback'] is False:
					Log.warn('No smbios found, skipping')
					continue
			result.append(vm)
		return result

	def is_expired(snap, last=False):
		splited = snap.split(';')
		created_at = dateutil.parser.parse(splited[-1])
		profile = splited[-3]
		value = int(splited[-2])
		if profile == 'daily':
			expiration = datetime.timedelta(days=value)
		elif profile == 'hourly':
			expiration = datetime.timedelta(hours=value)
		else:
			Log.warn('Unknown profile found, no action taken: %s' % (profile,))
			return False

		expired_at = created_at + expiration
		if last is True:
			expired_at += datetime.timedelta(days=config['extra_retention_time'])

		now = datetime.datetime.now()
		if expired_at > now:
			return False
		return True

	def create_snap_px(self, vm):
		try:
			# We freeze the VM once, thus create all snaps at the same time
			# Exports are done after thawing, because it it time-consuming,
			# and we must not keep the VM frozen more than necessary
			self.px.freeze(vm['node'], vm)

			for disk in vm['to_backup']:
				ceph = self.px.ceph_storage[disk['ceph']]
				bck = Bck(disk['ceph'], ceph, disk['rbd'], vm=vm, adapter=disk['adapter'])

				profiles = self.__fetch_profiles(vm, disk)
				self.__create_snap(bck, profiles)
		except Exception as e:
			Log.warn('create_snaps: %s thrown while processing %s' % (e, vm))

		try:
			self.px.thaw(vm['node'], vm)
		except:
			Log.warn('create_snaps: thaw failed while processing %s' % (vm,))

	def create_snap_plain(self, rbd):
		bck = Bck(self.cluster['name'], self.ceph, rbd)
		self.__create_snap(bck, config['profiles'].items())

	def __create_snap(self, bck, profiles):
		for profile, value in profiles:
			if not bck.check_profile(profile):
				continue
			dest, last_snap, snap_name = bck.make_snap(profile, value['count'])
			if dest is not None:
				queue.put({
						'dest': dest,
						'last_snap': last_snap,
						'snap_name': snap_name,
						'backup': bck,
				})

	def create_snaps(self):
		items = self.list_items()
		with multiprocessing.Pool(config['snapshooter_processes']) as pool:
			if self.cluster['type'] == 'proxmox':
				function = self.create_snap_px
			else:
				function = self.create_snap_plain
			for i in pool.imap_unordered(function, items):
				pass

	def expire_plop(ceph, rbd):
		snaps = ceph.snap(rbd)
		try:
			snaps.pop()  # We must never delete the latest snapshot
		except IndexError:
			# No snap found .. ?!
			return

		by_profile = {}
		for snap in snaps:
			tmp = snap.split(';')
			if tmp[1] not in by_profile:
				by_profile[tmp[1]] = list()
			i = by_profile[tmp[1]]
			i.append(snap)

		to_del = list()
		for profile, snaps in by_profile.items():
			try:
				profile = config['profiles'][profile]
			except KeyError:
				# Profile no longer exists, we can drop all these snaps
				to_del += snaps
				continue
			try:
				max_on_live = profile['max_on_live']
			except KeyError:
				max_on_live = 1

			for _ in range(0, max_on_live):
				try:
					snaps.pop()
				except IndexError:
					# We do not have enough snaps on live
					# snaps is now an empty list, nothing to delete
					break

			to_del += snaps
		for i in to_del:
			ceph.rm_snap(rbd, i)

	def expire_live(self):
		items = self.list_items()
		with multiprocessing.Pool(config['snapshooter_processes']) as pool:
			for i in pool.imap_unordered(self.expire_item, items):
				pass

	def expire_item(self, item):
		try:
			if self.cluster['type'] == 'proxmox':
				for disk in item['to_backup']:
					ceph = self.px.ceph_storage[disk['ceph']]
					Bidule.expire_plop(ceph, disk['rbd'])
			else:
				Bidule.expire_plop(self.ceph, item)
		except Exception as e:
			Log.warn('Expire_live on %s : %s' % (item, e))

	def expire_backup(i):
		ceph = i['ceph']
		image = i['image']

		snaps = ceph.backup.snap(image)
		try:
			# Pop the last snapshot
			# We will take care of it later
			last = snaps.pop()
		except IndexError:
			# We found an image without snapshot
			# Someone is messing around, or this is a bug
			# Anyway, the image can be deleted
			ceph.backup.rm(image)
			return

		for snap in snaps:
			if not Bidule.is_expired(snap):
				continue
			ceph.backup.rm_snap(image, snap)

		snaps = ceph.backup.snap(image)
		if len(snaps) == 1:
			if Bidule.is_expired(last, last=True):
				ceph.backup.rm_snap(image, snaps[0])

		if len(ceph.backup.snap(image)) == 0:
			Log.info('%s has no snapshot left, deleting' % (image,))
			ceph.backup.rm(image)


class Producer():
	def __init__(self, queue):
		self.queue = queue

	def __call__(self):
		Log.info('producer: start')
		try:
			self.__work__()
		except Exception as e:
			Log.error('Producer: %s' % (e,))

		# We send one None per download_worker
		# That way, all of them shall die
		for i in range(0, config['download_worker']):
			try:
				self.queue.put(None)
			except Exception as e:
				Log.error('Producer: cannot end a download_worker! This is a critical bug, we will never die')

		Log.info('producer: end')

	def __work__(self):
		for cluster in config['live_clusters']:
			Log.info('Backuping %s: %s' % (cluster['type'], cluster['name']))
			bidule = Bidule(cluster, self.queue)
			bidule.create_snaps()

		Log.info('work ok')


class Consumer():
	def __init__(self, queue):
		self.queue = queue

	def __call__(self):
		Log.info('consumer: start')
		try:
			self.__work__()
		except Exception as e:
			Log.error('Consumer: %s' % (e,))
		Log.info('consumer: end')

	def __work__(self):
		while True:
			snap = self.queue.get()
			Log.info(snap)
			if snap is None:
				break

			try:
				backup = snap['backup']
				backup.dl_snap(snap['snap_name'], snap['dest'], snap['last_snap'])
			except Exception as e:
				Log.error(e)


if __name__ == '__main__':
	if len(sys.argv) < 2:
		pretty.usage()

	try:
		action = sys.argv[1]
	except IndexError:
		Log.warn('nothing to do')
		exit(1)

	if action not in ('backup', 'check', 'check-snap', 'ls', 'list-mapped',
			'map', 'unmap'):
		Log.error('Action %s unknown' % (action,))
		pretty.usage()

	if action in ('check', 'check-snap'):
		result = list()

		for cluster in config['live_clusters']:
			Log.info('Checking %s: %s' % (cluster['type'], cluster['name']))
			if cluster['type'] == 'proxmox':
				check = CheckProxmox(cluster)
			else:
				check = CheckPlain(cluster)
			if action == 'check':
				ret = check.check()
			else:
				ret = check.check_snap()
			result += ret

		if len(result) > 0:
			for err in result:
				Log.error(err)
			Log.info('At the end, we failed: %s errors found' % (len(result),))
		exit(len(result))

	if action == 'backup':
		Log.info('Starting backup ..')

		manager = multiprocessing.Manager()
		atexit.register(manager.shutdown)
		queue = manager.Queue()
		producer = multiprocessing.Process(target=Producer(queue))
		atexit.register(producer.terminate)
		producer.start()

		download_workers = list()
		for i in range(0, config['download_worker']):
			pid = multiprocessing.Process(target=Consumer(queue))
			atexit.register(pid.terminate)
			download_workers.append(pid)
			pid.start()

		# Workers will exit upon a None reception
		# When all of them are done, we are done
		for pid in download_workers:
			pid.join()

		manager.shutdown()

		Log.info('Le manager est mort!')
		for cluster in config['live_clusters']:
			Log.info('Expire snapshots from live %s: %s' % (cluster['type'], cluster['name']))
			bidule = Bidule(cluster, None)
			bidule.expire_live()
		Log.info('expire terminé')

		Log.info('Expiring our snapshots')
		# Dummy Ceph object used to retrieve the real backup Object
		ceph = Ceph(None)

		data = list()
		for i in ceph.backup.ls():
			data.append({'ceph': ceph, 'image': i})
		with multiprocessing.Pool(config['expire_processes']) as pool:
			for i in pool.imap_unordered(Bidule.expire_backup, data):
				pass
		exit(0)

	try:
		rbd = sys.argv[2]
	except:
		rbd = None

	try:
		snap = sys.argv[3]
	except:
		snap = None

	restore = Restore(rbd, snap)
	if action == 'ls':
		data = restore.ls()
		if rbd is None:
			pt = pretty.Pt(['Ident', 'Disk', 'UUID'])

			for i in data:
				row = [i['ident'], i['disk'], i['uuid']]
				pt.add_row(row)
		else:
			pt = pretty.Pt(['Creation date', 'UUID'])

			for i in data:
				row = [i['creation'], i['uuid']]
				pt.add_row(row)
		print(pt)
	elif action == 'list-mapped':
		data = restore.list_mapped()
		pt = pretty.Pt(['rbd', 'snap', 'mount'])
		for i in data:
			pt.add_row([i['parent_image'], i['parent_snap'], i['mountpoint']])
		print(pt)
	elif action == 'map':
		if rbd is None or snap is None:
			pretty.usage()
		restore.mount()
	elif action == 'unmap':
		if rbd is None or snap is None:
			pretty.usage()
		restore.umount()
